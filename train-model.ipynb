{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giovanni\\Miniconda3\\envs\\keras\\lib\\site-packages\\keras_applications\\mobilenet_v2.py:295: UserWarning: MobileNet shape is undefined. Weights for input shape(224, 224) will be loaded.\n",
      "  warnings.warn('MobileNet shape is undefined.'\n"
     ]
    }
   ],
   "source": [
    "base_model=MobileNetV2(weights='imagenet', include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(512,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(512,activation='relu')(x) #dense layer 2\n",
    "preds=Dense(3,activation='softmax')(x) #final layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                 validation_split=0.3) #included in our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory('../data/',\n",
    "                                                    target_size=(224,224),\n",
    "                                                    color_mode='rgb',\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    subset='training',\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = train_datagen.flow_from_directory('../data/',\n",
    "                                                  target_size=(224,224),\n",
    "                                                  color_mode='rgb',\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  subset='validation',\n",
    "                                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint('model_chkpt/weights.{epoch:02d}_{val_loss:.4f}_{val_acc:.4f}.h5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 16s 730ms/step - loss: 0.7760 - acc: 0.6307 - val_loss: 0.6247 - val_acc: 0.7778\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 4s 175ms/step - loss: 0.3839 - acc: 0.8410 - val_loss: 0.4369 - val_acc: 0.8209\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 4s 160ms/step - loss: 0.1792 - acc: 0.9545 - val_loss: 0.3042 - val_acc: 0.8806\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 5s 207ms/step - loss: 0.1006 - acc: 0.9716 - val_loss: 0.2678 - val_acc: 0.8657\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 4s 196ms/step - loss: 0.0740 - acc: 0.9716 - val_loss: 0.2499 - val_acc: 0.8955\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 4s 185ms/step - loss: 0.0714 - acc: 0.9829 - val_loss: 0.2495 - val_acc: 0.9254\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 4s 204ms/step - loss: 0.0889 - acc: 0.9659 - val_loss: 0.1931 - val_acc: 0.9254\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 4s 172ms/step - loss: 0.1129 - acc: 0.9659 - val_loss: 0.3572 - val_acc: 0.8358\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 4s 188ms/step - loss: 0.0649 - acc: 0.9772 - val_loss: 0.2426 - val_acc: 0.8657\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 5s 208ms/step - loss: 0.1279 - acc: 0.9604 - val_loss: 0.3277 - val_acc: 0.8806\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 4s 170ms/step - loss: 0.0476 - acc: 0.9943 - val_loss: 0.2889 - val_acc: 0.8611\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 4s 196ms/step - loss: 0.1217 - acc: 0.9548 - val_loss: 0.3459 - val_acc: 0.9104\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 5s 212ms/step - loss: 0.1924 - acc: 0.9317 - val_loss: 0.2334 - val_acc: 0.8657\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 0.0646 - acc: 0.9772 - val_loss: 0.5137 - val_acc: 0.7761\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 4s 168ms/step - loss: 0.1952 - acc: 0.9261 - val_loss: 0.3172 - val_acc: 0.8657\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 4s 180ms/step - loss: 0.0588 - acc: 0.9716 - val_loss: 0.4400 - val_acc: 0.8358\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 5s 206ms/step - loss: 0.0818 - acc: 0.9602 - val_loss: 0.3486 - val_acc: 0.8209\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 5s 210ms/step - loss: 0.0307 - acc: 0.9886 - val_loss: 0.2167 - val_acc: 0.8955\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 4s 171ms/step - loss: 0.0595 - acc: 0.9886 - val_loss: 0.2888 - val_acc: 0.8657\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 4s 177ms/step - loss: 0.0620 - acc: 0.9716 - val_loss: 0.4026 - val_acc: 0.8358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14c690be400>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch = train_generator.samples // batch_size,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps = val_generator.samples // batch_size,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=20,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model('model_chkpt/weights.07_0.1931_0.9254.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfjs.converters.save_keras_model(best_model, 'model_output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31375   , 0.00212419, 0.68412584]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.random.rand(1, 224, 224, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
